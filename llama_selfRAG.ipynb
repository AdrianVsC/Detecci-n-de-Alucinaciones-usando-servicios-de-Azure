{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anatomy.txt', 'r', encoding='utf-8')as f:\n",
    "          file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "          chunk_size = 600,\n",
    "          chunk_overlap = 0,\n",
    "          length_function = len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import AzureSearch\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "from azure.core.exceptions import ServiceRequestError, ServiceResponseError\n",
    "\n",
    "documentos = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "try:\n",
    "    chunk_store = AzureSearch.from_documents(\n",
    "        azure_search_endpoint='https://hackathon-storage.search.windows.net',\n",
    "        azure_search_key='Q4pGUsJ9fWNNdHpkkEDve9rBXGIYz1DNWK9b7LENFAAzSeAP8Yqb',\n",
    "        documents=documentos,\n",
    "        embedding=OllamaEmbeddings(model='nomic-embed-text:latest')\n",
    "    )\n",
    "except (ServiceRequestError, ServiceResponseError) as e:\n",
    "    print(f\"Error de conexión con Azure Search: {e}\")\n",
    "    # Manejo alternativo, como usar un almacenamiento local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_vector = chunk_store.as_retriever(search_type=\"hybrid\")  # Configuración básica del retriever\n",
    "\n",
    "# Pasa `k` directamente en la consulta\n",
    "retrieved_docs = chunks_vector.invoke(\"what is MDT?\", k=1)\n",
    "\n",
    "# Imprime los resultados\n",
    "print(\"Documentos recuperados:\", retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = chunk_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Realiza una consulta\n",
    "query = \"What is AI?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "# Inspecciona los resultados\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"ID: {doc.metadata.get('id')}, Contenido: {doc.page_content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"tinyllama:1.1b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0,\n",
    "    format=\"json\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Crea un prompt más explícito\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Contexto: {context}\\n\\nPregunta: {question}\\n\\nResponde basándote en el contexto proporcionado:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = chunks_vector.invoke('what is MDT?')\n",
    "print(\"Documentos recuperados:\", retrieved_docs)\n",
    "\n",
    "# Prueba directa del modelo\n",
    "try:\n",
    "    response = llm.invoke(\n",
    "        prompt.format(\n",
    "            context=retrieved_docs, \n",
    "            question='what is MDT?'\n",
    "        )\n",
    "    )\n",
    "    print(\"Respuesta del modelo:\", response)\n",
    "except Exception as e:\n",
    "    print(f\"Error al invocar el modelo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "semantic_rag_chain = (\n",
    "          {'context':chunks_vector , 'question': RunnablePassthrough()}\n",
    "          | prompt\n",
    "          | llm\n",
    "          | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_rag_chain.invoke('what is MDT? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método alternativo de invocación\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = llm.invoke([\n",
    "    HumanMessage(content=\"Contexto: [lista de documentos recuperados]\\n\\nPregunta: What is MDT?\")\n",
    "])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['AzureSearch', 'OllamaEmbeddings'] vectorstore=<langchain_community.vectorstores.azuresearch.AzureSearch object at 0x0000021B22FBDD10>\n",
      "messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: What is MDT? \\nContext: tags=['AzureSearch', 'OllamaEmbeddings'] vectorstore=<langchain_community.vectorstores.azuresearch.AzureSearch object at 0x0000021B22FBDD10> \\nAnswer:\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chunks_vector)\n",
    "print(prompt.invoke({\"context\": chunks_vector, \"question\": \"What is MDT?\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
