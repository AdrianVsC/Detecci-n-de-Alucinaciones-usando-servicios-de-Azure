{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "azure_search_endpoint = os.getenv('AZURE_SEARCH_ENDPOINT')\n",
    "azure_search_key = os.getenv('AZURE_SEARCH_KEY')\n",
    "azure_openai_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "openai_deployment = os.getenv('AZURE_OPENAI_DEPLOYMENT')\n",
    "openai_api_version = os.getenv('OPENAI_API_VERSION')\n",
    "azure_search_index = os.getenv('AZURE_SEARCH_INDEX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# Carga modelo NLP para analizar coherencia\n",
    "# nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "# Cargar texto y dividir en fragmentos\n",
    "with open('anatomy.txt', 'r', encoding='utf-8') as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(file)\n",
    "\n",
    "# Crear documentos\n",
    "documentos = [Document(page_content=chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import AzureSearch\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "# Crear índice de búsqueda vectorial en Azure\n",
    "chunk_store = AzureSearch.from_documents(\n",
    "    azure_search_endpoint=azure_search_endpoint,\n",
    "    azure_search_key=azure_search_key,\n",
    "    documents=documentos,\n",
    "    embedding=OllamaEmbeddings(model='nomic-embed-text:latest')\n",
    ")\n",
    "\n",
    "chunks_vector = chunk_store.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_vector = AzureSearch(\n",
    "          azure_search_endpoint=azure_search_endpoint,\n",
    "          azure_search_key=azure_search_key,\n",
    "          index_name=azure_search_index,\n",
    "          embedding_function=OllamaEmbeddings(model='nomic-embed-text:latest')\n",
    ")\n",
    "\n",
    "chunks_vector = chunk_store.as_retriever(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Section 2. Information about the Mission of Project Gutenberg™\n",
      "\n",
      "Project Gutenberg™ is synonymous with the free distribution of\n",
      "electronic works in formats readable by the widest variety of\n",
      "computers including obsolete, old, middle-aged and new computers. It\n",
      "exists because of the efforts of hundreds of volunteers and donations\n",
      "from people in all walks of life.\n"
     ]
    }
   ],
   "source": [
    "# a_vector = a.as_retriever(k=1)\n",
    "# docs = a_vector.invoke('qué contiene el libro proyecto gutenberg?')\n",
    "# for doc in docs:\n",
    "#           print(doc.page_content)\n",
    "b = 'qué contiene el libro proyecto gutenberg?'\n",
    "def buscar_documentos(query):\n",
    "    vector = a.as_retriever(k = 1)\n",
    "    docs = vector.invoke(query)\n",
    "    documentos = [doc.page_content for doc in docs]\n",
    "    return documentos\n",
    "\n",
    "info = buscar_documentos(b)\n",
    "for i, doc in enumerate(info, start=1):\n",
    "    print(f\"{i}. {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_vector = chunk_store.as_retriever(k=1)\n",
    "question = \"qué contiene el libro proyecto gutenberg?\"\n",
    "docs = chunks_vector.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 2. Information about the Mission of Project Gutenberg™\n",
      "\n",
      "Project Gutenberg™ is synonymous with the free distribution of\n",
      "electronic works in formats readable by the widest variety of\n",
      "computers including obsolete, old, middle-aged and new computers. It\n",
      "exists because of the efforts of hundreds of volunteers and donations\n",
      "from people in all walks of life.\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "          print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\argui\\OneDrive\\Escritorio\\Hackathon_IA\\env\\Lib\\site-packages\\langsmith\\client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El libro Proyecto Gutenberg contiene obras electrónicas de distribución gratuita en formatos compatibles con una amplia variedad de computadoras.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=openai_deployment,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=openai_api_key,\n",
    "    api_version=openai_api_version,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\argui\\OneDrive\\Escritorio\\Hackathon_IA\\env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import Literal\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "#Literal\n",
    "    binary_score: bool = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=openai_deployment,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=openai_api_key,\n",
    "    api_version=openai_api_version,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score True or False. True means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "from langchain_community.vectorstores import AzureSearch\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "import numpy as np\n",
    "\n",
    "class RAGHallucinationMetrics:\n",
    "    def __init__(self, azure_search_endpoint: str, azure_search_key: str):\n",
    "        \"\"\"Inicializa el sistema de métricas de alucinaciones con componentes RAG.\"\"\"\n",
    "        self.results_db = self.load_or_create_db()\n",
    "        self.azure_search_endpoint = azure_search_endpoint\n",
    "        self.azure_search_key = azure_search_key\n",
    "        \n",
    "    def load_or_create_db(self) -> pd.DataFrame:\n",
    "        \"\"\"Carga o crea la base de datos de resultados.\"\"\"\n",
    "        try:\n",
    "            return pd.read_csv('rag_hallucination_results.csv')\n",
    "        except FileNotFoundError:\n",
    "            return pd.DataFrame({\n",
    "                'timestamp': [],\n",
    "                'question': [],\n",
    "                'generation': [],\n",
    "                'is_hallucination': [],\n",
    "                'retrieval_score': [],\n",
    "                'context_relevance': [],\n",
    "                'num_retrieved_docs': [],\n",
    "                'category': []\n",
    "            })\n",
    "    \n",
    "    def calculate_retrieval_score(self, docs: List) -> float:\n",
    "        \"\"\"Calcula un score de calidad de recuperación basado en los documentos.\"\"\"\n",
    "        if not docs:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calcula la puntuación basada en la cantidad y relevancia de documentos\n",
    "        scores = [getattr(doc, 'score', 0.5) for doc in docs]  # 0.5 por defecto si no hay score\n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def evaluate_context_relevance(self, question: str, docs: List) -> float:\n",
    "        \"\"\"Evalúa la relevancia del contexto recuperado para la pregunta.\"\"\"\n",
    "        if not docs:\n",
    "            return 0.0\n",
    "        \n",
    "        # Implementa lógica de evaluación de relevancia\n",
    "        # Por ejemplo, similitud coseno entre pregunta y documentos\n",
    "        return sum(getattr(doc, 'score', 0.5) for doc in docs) / len(docs)\n",
    "    \n",
    "    def add_result(self, question: str, generation: str, docs: List, \n",
    "                   is_hallucination: bool, category: str = None):\n",
    "        \"\"\"Añade un nuevo resultado a la base de datos con métricas RAG.\"\"\"\n",
    "        retrieval_score = self.calculate_retrieval_score(docs)\n",
    "        context_relevance = self.evaluate_context_relevance(question, docs)\n",
    "        \n",
    "        new_row = pd.DataFrame([{\n",
    "            'timestamp': datetime.now(),\n",
    "            'question': question,\n",
    "            'generation': generation,\n",
    "            'is_hallucination': is_hallucination,\n",
    "            'retrieval_score': retrieval_score,\n",
    "            'context_relevance': context_relevance,\n",
    "            'num_retrieved_docs': len(docs),\n",
    "            'category': category\n",
    "        }])\n",
    "        \n",
    "        self.results_db = pd.concat([self.results_db, new_row], ignore_index=True)\n",
    "        self.results_db.to_csv('rag_hallucination_results.csv', index=False)\n",
    "    \n",
    "    def calculate_metrics(self) -> Dict:\n",
    "        \"\"\"Calcula métricas detalladas sobre el sistema RAG y alucinaciones.\"\"\"\n",
    "        if len(self.results_db) == 0:\n",
    "            return {}\n",
    "        \n",
    "        metrics = {\n",
    "            'total_queries': len(self.results_db),\n",
    "            'hallucination_rate': (self.results_db['is_hallucination'].mean() * 100),\n",
    "            'avg_retrieval_score': self.results_db['retrieval_score'].mean(),\n",
    "            'avg_context_relevance': self.results_db['context_relevance'].mean(),\n",
    "            'avg_docs_retrieved': self.results_db['num_retrieved_docs'].mean(),\n",
    "            'correlation_retrieval_hallucination': self.results_db['retrieval_score'].corr(self.results_db['is_hallucination'])\n",
    "        }\n",
    "        \n",
    "        if 'category' in self.results_db.columns:\n",
    "            metrics['category_breakdown'] = self.results_db.groupby('category')[\n",
    "                ['is_hallucination', 'retrieval_score']].mean().to_dict()\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "def create_rag_dashboard():\n",
    "    \"\"\"Crea un dashboard en Streamlit para visualizar las métricas RAG.\"\"\"\n",
    "    st.title(\"Dashboard de RAG y Detección de Alucinaciones\")\n",
    "    \n",
    "    # Configuración\n",
    "    with st.sidebar:\n",
    "        st.header(\"Configuración\")\n",
    "        azure_search_endpoint = st.text_input(\"Azure Search Endpoint\")\n",
    "        azure_search_key = st.text_input(\"Azure Search Key\", type=\"password\")\n",
    "        \n",
    "        if not all([azure_search_endpoint, azure_search_key]):\n",
    "            st.warning(\"Por favor, completa la configuración de Azure Search\")\n",
    "            return\n",
    "    \n",
    "    # Inicializar sistema de métricas\n",
    "    metrics = RAGHallucinationMetrics(azure_search_endpoint, azure_search_key)\n",
    "    \n",
    "    # Métricas RAG\n",
    "    st.header(\"Métricas RAG\")\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    \n",
    "    general_metrics = metrics.calculate_metrics()\n",
    "    if general_metrics:\n",
    "        col1.metric(\"Tasa de Alucinaciones\", f\"{general_metrics['hallucination_rate']:.2f}%\")\n",
    "        col2.metric(\"Score Promedio de Recuperación\", f\"{general_metrics['avg_retrieval_score']:.2f}\")\n",
    "        col3.metric(\"Documentos Promedio\", f\"{general_metrics['avg_docs_retrieved']:.1f}\")\n",
    "        \n",
    "        # Gráfico de correlación\n",
    "        st.subheader(\"Correlación entre Recuperación y Alucinaciones\")\n",
    "        correlation_data = metrics.results_db[['retrieval_score', 'is_hallucination']]\n",
    "        fig_corr = px.scatter(correlation_data, \n",
    "                            x='retrieval_score', \n",
    "                            y='is_hallucination',\n",
    "                            title=\"Relación entre Score de Recuperación y Alucinaciones\")\n",
    "        st.plotly_chart(fig_corr)\n",
    "        \n",
    "        # Análisis temporal\n",
    "        st.subheader(\"Tendencia Temporal\")\n",
    "        fig_time = px.line(metrics.results_db, \n",
    "                          x='timestamp', \n",
    "                          y=['retrieval_score', 'context_relevance'],\n",
    "                          title=\"Evolución de Scores en el Tiempo\")\n",
    "        st.plotly_chart(fig_time)\n",
    "        \n",
    "        # Tabla de últimas consultas\n",
    "        st.subheader(\"Últimas Consultas\")\n",
    "        st.dataframe(metrics.results_db.tail(10)[\n",
    "            ['timestamp', 'question', 'is_hallucination', 'retrieval_score', 'num_retrieved_docs']])\n",
    "\n",
    "def evaluate_rag_response(\n",
    "    question: str,\n",
    "    chunk_store: AzureSearch,\n",
    "    rag_chain,\n",
    "    hallucination_grader,\n",
    "    metrics: RAGHallucinationMetrics,\n",
    "    category: str = None\n",
    "):\n",
    "    \"\"\"Evalúa una respuesta RAG y actualiza métricas.\"\"\"\n",
    "    # Recuperar documentos\n",
    "    docs = chunk_store.as_retriever().get_relevant_documents(question)\n",
    "    \n",
    "    # Generar respuesta\n",
    "    generation = rag_chain.invoke({\n",
    "        \"context\": \"\\n\\n\".join(doc.page_content for doc in docs),\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    # Evaluar alucinación\n",
    "    hallucination_result = hallucination_grader.invoke({\n",
    "        \"documents\": docs,\n",
    "        \"generation\": generation\n",
    "    })\n",
    "    \n",
    "    # Almacenar resultado\n",
    "    metrics.add_result(\n",
    "        question=question,\n",
    "        generation=generation,\n",
    "        docs=docs,\n",
    "        is_hallucination=not hallucination_result.binary_score,\n",
    "        category=category\n",
    "    )\n",
    "    \n",
    "    return generation, hallucination_result.binary_score, docs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_rag_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat iniciado. Escribe 'salir' para terminar.\n",
      "\n",
      "Assistant: ¡Hola! ¿Cómo puedo ayudarte hoy?\n",
      "¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "\n",
    "def chat_with_azure_gpt(\n",
    "    deployment_name,\n",
    "    api_key,\n",
    "    azure_endpoint,\n",
    "    api_version\n",
    "):\n",
    "    \n",
    "    # Configurar el chat\n",
    "    chat = AzureChatOpenAI(\n",
    "      azure_deployment=openai_deployment,\n",
    "      azure_endpoint=azure_openai_endpoint,\n",
    "      api_key=openai_api_key,\n",
    "      api_version=openai_api_version,\n",
    "      temperature=0,\n",
    "      max_tokens=None,\n",
    "      timeout=None,\n",
    "      max_retries=2,\n",
    "    )\n",
    "    \n",
    "    print(\"Chat iniciado. Escribe 'salir' para terminar.\")\n",
    "    \n",
    "    while True:\n",
    "        # Obtener input del usuario\n",
    "        user_input = input(\"\\nTú: \")\n",
    "        \n",
    "        # Verificar si el usuario quiere salir\n",
    "        if user_input.lower() == 'salir':\n",
    "            print(\"¡Hasta luego!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Crear el mensaje y obtener la respuesta\n",
    "            messages = user_input\n",
    "            response = chat.invoke(messages)\n",
    "            \n",
    "            # Imprimir la respuesta\n",
    "            print(\"\\nAssistant:\", response.content)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    DEPLOYMENT_NAME = openai_deployment\n",
    "    API_KEY = azure_search_key\n",
    "    AZURE_ENDPOINT = azure_search_endpoint\n",
    "    OPENAI_API_VERSION = openai_api_version\n",
    "    \n",
    "    chat_with_azure_gpt(\n",
    "        deployment_name=DEPLOYMENT_NAME,\n",
    "        api_key=API_KEY,\n",
    "        azure_endpoint=AZURE_ENDPOINT,\n",
    "        api_version=OPENAI_API_VERSION\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=openai_deployment,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=openai_api_key,\n",
    "    api_version=openai_api_version,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = chunks_vector.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=openai_deployment,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=openai_api_key,\n",
    "    api_version=openai_api_version,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "question = \"agent memory\"\n",
    "docs = chunks_vector.invoke(question)\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# Modelo de LLM para evaluación de relevancia\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Puntuación binaria para la alucinación presente en la respuesta de la generación.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"La respuesta se basa en los hechos, «sí» o «no\"\n",
    "    )\n",
    " \n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=openai_deployment,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=openai_api_key,\n",
    "    api_version=openai_api_version,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "system = \"\"\"Usted es un calificador que evalúa si una generación de LLM está fundamentada / apoyada por un conjunto de hechos recuperados. \\n \n",
    "     Da una puntuación binaria 'sí' o 'no'. Sí' significa que la respuesta está basada / apoyada por el conjunto de hechos.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {document} \\n\\n LLM generation: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "generation = hallucination_grader.invoke({\"context\": docs, \"question\": question})\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import AzureSearch\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Carga modelo NLP para analizar coherencia\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "# Cargar texto y dividir en fragmentos\n",
    "with open('anatomy.txt', 'r', encoding='utf-8') as f:\n",
    "    file = f.read()\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(file)\n",
    "\n",
    "# Crear documentos\n",
    "documentos = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Crear índice de búsqueda vectorial en Azure\n",
    "chunk_store = AzureSearch.from_documents(\n",
    "    azure_search_endpoint=azure_search_endpoint,\n",
    "    azure_search_key=azure_search_key,\n",
    "    documents=documentos,\n",
    "    embedding=OllamaEmbeddings(model='nomic-embed-text:latest')\n",
    ")\n",
    "\n",
    "chunks_vector = chunk_store.as_retriever()\n",
    "\n",
    "# Modelo de LLM para evaluación de relevancia\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score: str = Field(\n",
    "        description=\"Los documentos son relevantes para la pregunta 'sí' o 'no'.\"\n",
    "    )\n",
    " \n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=openai_deployment,\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=openai_api_key,\n",
    "    api_version=openai_api_version,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt para evaluar relevancia\n",
    "system = \"\"\"Eres un calificador que evalúa la relevancia de un documento recuperado con respecto a una pregunta del usuario.\n",
    "Si el documento contiene palabras clave o significados semánticos relacionados con la pregunta del usuario, califíquelo como pertinente.\n",
    "Dar una puntuación binaria «sí» o «no» para indicar si el documento es pertinente para la pregunta.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "# Función para analizar coherencia interna\n",
    "def analizar_coherencia(texto):\n",
    "    \"\"\"Analiza la coherencia interna de un texto.\"\"\"\n",
    "    doc = nlp(texto)\n",
    "    inconsistencias = []\n",
    "    for sent in doc.sents:\n",
    "        tokens = [token.text for token in sent if token.dep_ == \"neg\"]\n",
    "        if tokens:\n",
    "            inconsistencias.append(sent.text)\n",
    "    return inconsistencias\n",
    "\n",
    "# Orquestador para detectar alucinaciones y problemas de coherencia\n",
    "def evaluar_texto(question):\n",
    "    \"\"\"Evalúa un texto en busca de alucinaciones y problemas de coherencia.\"\"\"\n",
    "    # Recuperar documentos relevantes\n",
    "    docs = chunks_vector.get_relevant_documents(question)\n",
    "    if not docs:\n",
    "        return {\n",
    "            \"alucinaciones_probabilidad\": 1.0,\n",
    "            \"comentarios\": \"No se encontraron documentos relevantes para la pregunta.\",\n",
    "            \"acciones_recomendadas\": [\"Agregar documentos relevantes al índice de Azure.\"]\n",
    "        }\n",
    "\n",
    "    # Evaluar relevancia con el modelo\n",
    "    relevancia = []\n",
    "    for doc in docs:\n",
    "        doc_txt = doc.page_content\n",
    "        result = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "        relevancia.append(result[\"binary_score\"] == \"yes\")\n",
    "\n",
    "    # Detectar alucinaciones\n",
    "    probabilidad_alucinacion = 1.0 - sum(relevancia) / len(relevancia) if relevancia else 1.0\n",
    "\n",
    "    # Analizar coherencia de los documentos recuperados\n",
    "    inconsistencias = []\n",
    "    for doc in docs:\n",
    "        inconsistencias.extend(analizar_coherencia(doc.page_content))\n",
    "\n",
    "    # Generar reporte\n",
    "    comentarios = []\n",
    "    if probabilidad_alucinacion > 0.5:\n",
    "        comentarios.append(\"La información parece ficticia o no respaldada por los documentos.\")\n",
    "    if inconsistencias:\n",
    "        comentarios.append(f\"Inconsistencias detectadas en los documentos: {', '.join(inconsistencias)}\")\n",
    "\n",
    "    return {\n",
    "        \"alucinaciones_probabilidad\": probabilidad_alucinacion,\n",
    "        \"comentarios\": \" \".join(comentarios),\n",
    "        \"acciones_recomendadas\": [\n",
    "            \"Verificar el contenido manualmente.\",\n",
    "            \"Agregar más datos relevantes al índice de Azure.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Ejemplo de uso\n",
    "question = \"agent memory\"\n",
    "reporte = evaluar_texto(question)\n",
    "print(reporte)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
